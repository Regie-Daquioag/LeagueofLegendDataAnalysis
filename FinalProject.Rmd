---
title: "Final Project"
author: "Regie Daquioag and Carlos Huizar"
date: "May 03, 2018"
output: html_document
---

<!-- change echo=FALSE to echo=TRUE to show code -->
```{r global_options, include=FALSE}
library(rpart)
library(rpart.plot)
library(maptree)
library(e1071)
set.seed(123)
knitr::opts_chunk$set(prompt=TRUE, comment="", echo=TRUE)
dat = read.csv("https://raw.githubusercontent.com/Regie-Daquioag/LeagueofLegendDataAnalysis/master/games.csv")
```

## Project Introduction

In this project we are going to explore the influences that help teams win games in League of Legends. In LoL there are various of factors that can help win the game such as getting kills and taking objectives on the map. Teams use these to obtain gold that helps make their character stronger and the winner is decided by whoever takes the opponents base first. We are going to explore the various of factors that can help win the game and decside which are the most important to focus on. 

## Project Data

Data Source: https://www.kaggle.com/datasnaek/league-of-legends/data


## Data cleaning and preprocessing


```{r}
#Erase the season ID as it provides no statistical value for the set
dat$seasonId = NULL

#Champion Bans are set to NULL to help run the data set for efficiently
dat$t1_ban1 = NULL
dat$t1_ban2 = NULL
dat$t1_ban3 = NULL
dat$t1_ban4 = NULL
dat$t1_ban5 = NULL
dat$t2_ban1 = NULL
dat$t2_ban2 = NULL
dat$t2_ban3 = NULL
dat$t2_ban4 = NULL
dat$t2_ban5 = NULL

#Champion ID's are set to NULL because the different champions are not always in the same game and are wild predictors and we will not be using them in our analysis. 
dat$t1_champ1id = NULL
dat$t1_champ2id = NULL
dat$t1_champ3id = NULL
dat$t1_champ4id = NULL
dat$t1_champ5id = NULL
dat$t2_champ1id = NULL
dat$t2_champ2id = NULL
dat$t2_champ3id = NULL
dat$t2_champ4id = NULL
dat$t2_champ5id = NULL

#All summoner spells are set to NULL because they will not be used in the analysis of the data set.
dat$t1_champ1_sum1 = NULL
dat$t1_champ2_sum1 = NULL
dat$t1_champ3_sum1 = NULL
dat$t1_champ4_sum1 = NULL
dat$t1_champ5_sum1 = NULL
dat$t1_champ1_sum2 = NULL
dat$t1_champ2_sum2 = NULL
dat$t1_champ3_sum2 = NULL
dat$t1_champ4_sum2 = NULL
dat$t1_champ5_sum2 = NULL
dat$t2_champ1_sum1 = NULL
dat$t2_champ2_sum1 = NULL
dat$t2_champ3_sum1 = NULL
dat$t2_champ4_sum1 = NULL
dat$t2_champ5_sum1 = NULL
dat$t2_champ1_sum2 = NULL
dat$t2_champ2_sum2 = NULL
dat$t2_champ3_sum2 = NULL
dat$t2_champ4_sum2 = NULL
dat$t2_champ5_sum2 = NULL
```
##Data Exploration

The goal of these first barplots is to find out what feature is more impactful in games and what teams should be focusing on. 

*First Blood*

```{r}
colors = c("blue", "red")
colors2 = c("red", "blue")
tb = sort(table(dat[dat$firstBlood == "1",]$winner))
barplot(tb,horiz = T,las=2, cex.names = 0.5, xlab="Number of Wins", col = colors)
tb = sort(table(dat[dat$firstBlood == "2",]$winner))
barplot(tb,horiz = T,las=2, cex.names = 0.5, xlab="Number of Wins", col = colors2)

#Using these two bar plots we found that teams that had been able to secure first blood won 59% of the games that were played. This is without influence from other features so then now we will check the first tower.
```

*First Tower*

```{r}
tb = sort(table(dat[dat$firstTower == "1",]$winner))
barplot(tb,horiz = T,las=2, cex.names = 0.5, xlab="Number of Wins", col = colors)
tb = sort(table(dat[dat$firstTower == "2",]$winner))
barplot(tb,horiz = T,las=2, cex.names = 0.5, xlab="Number of Wins", col = colors2)

#Next we tested the first tower win rate and it is significantly larger than the first blood rates, increasing to a 70% win rate for teams that were able to collect the first tower gold. This makes a lot of sense because if a team takes that tower they now have tempo and can move to another part of the map to take another tower.

```

*First Blood and First Tower*

```{r}
tb = sort(table(dat[dat$firstBlood == "1" & dat$firstTower == "1",]$winner))
barplot(tb,horiz = T,las=2, cex.names = 0.5, xlab="Number of Wins", col = colors)
tb = sort(table(dat[dat$firstBlood == "1" & dat$firstTower == "2",]$winner))
barplot(tb,horiz = T,las=2, cex.names = 0.5, xlab="Number of Wins", col = colors2)

#Now if we combine the two features we increase the win percentage to 75%. It seems underwhelming compared to the difference with first tower and first blood. However, league is a team game are there are many more things that can influence the game, one of those being the player which we cannot account for.   
```  

##Histograms showing some predictors that willl be used
```{r}
par(mfrow=c(2,3))
colors = c("grey", "blue", "red")
barplot(table(dat$firstBlood), main="First Blood", col=colors)
barplot(table(dat$firstTower), main="First Tower", col=colors)
barplot(table(dat$firstInhibitor), main="First Inhibitor", col=colors)
barplot(table(dat$firstBaron), main="First Baron", col=colors)
barplot(table(dat$firstDragon), main="First Dragon", col=colors)
barplot(table(dat$firstRiftHerald), main="First Rift Herald", col=colors)
```

##Logistic Regression Model
```{r}
tr_rows=sample(nrow(dat), 0.75 * nrow(dat))
tr_dat = dat[tr_rows,]
te_dat = dat[-tr_rows,]

fit = glm(winner-1 ~ firstBlood + firstTower, data=tr_dat, family=binomial)

y = predict(fit, newdata=te_dat, type="response")
predicts = as.numeric(y > 0.55)
actuals = te_dat$winner-1
conf_mtx = table(predicts, actuals)
conf_mtx
rate = mean(predicts == actuals)
round(rate, 2)

plot(density(y[actuals == 1]), col="red", main="Double Density for League of Lengends Games")
lines(density(y[actuals == 0]), col="blue")
```

```{r}
prec_recall_summary = function(predicts, actuals) {
  thresh = seq(0, 1, length.out=65)
  prec_rec = data.frame()
  actuals = factor(as.numeric(actuals))
  for (th in thresh) {
    predicts = factor(as.numeric(y >= th), levels=c("0","1"))
    prec_rec = rbind(prec_rec, as.vector(table(predicts, actuals)))
  }
  names(prec_rec) = c("TN", "FP", "FN", "TP")
  prec_rec$precision = prec_rec$TP/(prec_rec$TP + prec_rec$FP)
  prec_rec$recall    = prec_rec$TP/(prec_rec$TP + prec_rec$FN)
  prec_rec$false_pos = prec_rec$FP/(prec_rec$FP + prec_rec$TN)
  prec_rec$true_pos = prec_rec$TP/(prec_rec$TP + prec_rec$FN)
  return(prec_rec)
}
prec_rec1 = prec_recall_summary(predicts, actuals)

plot(prec_rec1$true_pos ~ prec_rec1$false_pos, xlab="False Positive", ylab="True Positive", type="l", col="red4", main="ROC")
plot(prec_rec1$precision ~ prec_rec1$recall, xlab="Recall", ylab="Precision", type="l", col="red4", main="Precision-Recall Curve")
```

##Decision Tree: Classification
```{r}
fit = rpart(winner ~ ., data=tr_dat, method="class")
prp(fit, extra=1, varlen=-10, main="classification tree for blue/red team.", box.col=c("dodgerblue", "firebrick1")[fit$frame$yval])
predicted = predict(fit, te_dat, type="class")
actual = te_dat$winner
table(actual, predicted)
mean(actual == predicted)
```

Using this decision tree, we are able to find out which team won depending on the number of tower kills. There seems to be a consistent pattern with this because taking towers gives a significant amount of gold and that gold can be used to help the team get further ahead.

## Assessing the decision tree
```{r}
te_errs = c()
tr_errs = c()
te_actual = te_dat$winner
tr_sizes = seq(100, nrow(tr_dat), length.out=10)
 for (tr_size in tr_sizes) {
   tr_dat1 = tr_dat[1:tr_size,]
   tr_actual = tr_dat1$winner
   fit = rpart(winner ~ ., data=tr_dat1,method="class")
 
 #error on training set
   tr_predicted = predict(fit, tr_dat1, type="class")
   err = mean(tr_actual != tr_predicted)
   tr_errs = c(tr_errs, err)
   table(tr_actual, tr_predicted)
 
 # error on test set
   te_predicted = predict(fit, te_dat, type="class")
   err = mean(te_actual != te_predicted)
   te_errs = c(te_errs, err)
   table(te_actual, te_predicted)
 }
 # Plot learning curve here
 plot(tr_errs, type="b", col="red", xlab="Training Set Size", ylab="Classification Error", main=paste("Learning Curve"), ylim=c(.045,.09))
 lines(te_errs, type = "b", col = "blue")
 legend('topright', c("Train error", "Test error"), lty = c(1,1), lwd = c(2.5, 2.5), col = c("red", "blue"), cex=0.7)
```

Studying this learning curve shows that out lines are extremely close together, this means that we have a lot of bias. We can try and fix this by getting more features or trying to derive better features from the current data set that we have, however, obtaining more data will not help us with this.

